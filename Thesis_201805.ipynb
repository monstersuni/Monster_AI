{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "with sqlite3.connect('finance.sqlite') as db:\n",
    "    df_all=pandas.read_sql(\"select Open,High,Low,Price FROM MI_NATIONAL_INDEX  where IndexName='TAIEX' and Date <='2017-12-31' \",con=db)\n",
    "    df_test=pandas.read_sql(\"select Open,High,Low,Price FROM MI_NATIONAL_INDEX  where IndexName='TAIEX' and Date >'2017-12-31'\",con=db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9292.31</td>\n",
       "      <td>9292.31</td>\n",
       "      <td>9182.02</td>\n",
       "      <td>9274.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9209.93</td>\n",
       "      <td>9209.93</td>\n",
       "      <td>9043.44</td>\n",
       "      <td>9048.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9051.94</td>\n",
       "      <td>9108.66</td>\n",
       "      <td>9050.54</td>\n",
       "      <td>9080.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9154.03</td>\n",
       "      <td>9246.62</td>\n",
       "      <td>9154.03</td>\n",
       "      <td>9238.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9247.40</td>\n",
       "      <td>9284.57</td>\n",
       "      <td>9215.58</td>\n",
       "      <td>9215.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9198.02</td>\n",
       "      <td>9229.65</td>\n",
       "      <td>9178.30</td>\n",
       "      <td>9178.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9162.77</td>\n",
       "      <td>9253.82</td>\n",
       "      <td>9161.71</td>\n",
       "      <td>9231.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9235.90</td>\n",
       "      <td>9242.20</td>\n",
       "      <td>9161.71</td>\n",
       "      <td>9180.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9197.32</td>\n",
       "      <td>9219.43</td>\n",
       "      <td>9149.10</td>\n",
       "      <td>9165.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9245.77</td>\n",
       "      <td>9256.86</td>\n",
       "      <td>9109.46</td>\n",
       "      <td>9138.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9183.76</td>\n",
       "      <td>9247.09</td>\n",
       "      <td>9151.82</td>\n",
       "      <td>9174.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>9188.76</td>\n",
       "      <td>9259.31</td>\n",
       "      <td>9177.48</td>\n",
       "      <td>9251.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>9283.79</td>\n",
       "      <td>9319.71</td>\n",
       "      <td>9275.68</td>\n",
       "      <td>9319.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>9336.81</td>\n",
       "      <td>9386.56</td>\n",
       "      <td>9336.81</td>\n",
       "      <td>9369.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9422.00</td>\n",
       "      <td>9471.80</td>\n",
       "      <td>9422.00</td>\n",
       "      <td>9470.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9474.56</td>\n",
       "      <td>9478.53</td>\n",
       "      <td>9436.47</td>\n",
       "      <td>9477.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9505.56</td>\n",
       "      <td>9523.73</td>\n",
       "      <td>9471.19</td>\n",
       "      <td>9521.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9503.20</td>\n",
       "      <td>9515.29</td>\n",
       "      <td>9471.31</td>\n",
       "      <td>9510.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9479.36</td>\n",
       "      <td>9482.13</td>\n",
       "      <td>9403.75</td>\n",
       "      <td>9426.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9452.85</td>\n",
       "      <td>9466.65</td>\n",
       "      <td>9361.91</td>\n",
       "      <td>9361.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9368.83</td>\n",
       "      <td>9410.43</td>\n",
       "      <td>9352.19</td>\n",
       "      <td>9386.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>9394.47</td>\n",
       "      <td>9460.17</td>\n",
       "      <td>9394.47</td>\n",
       "      <td>9448.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9477.52</td>\n",
       "      <td>9533.88</td>\n",
       "      <td>9477.52</td>\n",
       "      <td>9513.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>9511.06</td>\n",
       "      <td>9517.42</td>\n",
       "      <td>9472.42</td>\n",
       "      <td>9512.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>9521.88</td>\n",
       "      <td>9521.88</td>\n",
       "      <td>9448.65</td>\n",
       "      <td>9456.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>9465.12</td>\n",
       "      <td>9465.12</td>\n",
       "      <td>9413.18</td>\n",
       "      <td>9421.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>9405.59</td>\n",
       "      <td>9435.07</td>\n",
       "      <td>9390.13</td>\n",
       "      <td>9393.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>9430.17</td>\n",
       "      <td>9488.72</td>\n",
       "      <td>9430.17</td>\n",
       "      <td>9462.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>9496.39</td>\n",
       "      <td>9500.35</td>\n",
       "      <td>9447.19</td>\n",
       "      <td>9496.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9500.04</td>\n",
       "      <td>9542.09</td>\n",
       "      <td>9499.97</td>\n",
       "      <td>9529.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>10706.79</td>\n",
       "      <td>10728.39</td>\n",
       "      <td>10656.18</td>\n",
       "      <td>10664.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>10688.22</td>\n",
       "      <td>10787.58</td>\n",
       "      <td>10687.26</td>\n",
       "      <td>10779.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>10815.40</td>\n",
       "      <td>10882.65</td>\n",
       "      <td>10801.10</td>\n",
       "      <td>10822.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>711</th>\n",
       "      <td>10837.61</td>\n",
       "      <td>10872.29</td>\n",
       "      <td>10809.09</td>\n",
       "      <td>10854.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>712</th>\n",
       "      <td>10847.79</td>\n",
       "      <td>10873.91</td>\n",
       "      <td>10833.78</td>\n",
       "      <td>10854.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>10836.82</td>\n",
       "      <td>10856.00</td>\n",
       "      <td>10746.74</td>\n",
       "      <td>10750.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>10733.56</td>\n",
       "      <td>10752.58</td>\n",
       "      <td>10692.53</td>\n",
       "      <td>10707.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>10735.24</td>\n",
       "      <td>10762.91</td>\n",
       "      <td>10697.62</td>\n",
       "      <td>10713.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>10623.20</td>\n",
       "      <td>10650.18</td>\n",
       "      <td>10560.44</td>\n",
       "      <td>10560.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>10591.62</td>\n",
       "      <td>10665.92</td>\n",
       "      <td>10495.15</td>\n",
       "      <td>10600.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>10617.27</td>\n",
       "      <td>10665.43</td>\n",
       "      <td>10565.68</td>\n",
       "      <td>10651.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>10607.62</td>\n",
       "      <td>10635.71</td>\n",
       "      <td>10540.74</td>\n",
       "      <td>10566.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>10541.03</td>\n",
       "      <td>10541.03</td>\n",
       "      <td>10376.65</td>\n",
       "      <td>10393.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>10404.42</td>\n",
       "      <td>10418.97</td>\n",
       "      <td>10322.76</td>\n",
       "      <td>10355.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>10406.35</td>\n",
       "      <td>10419.28</td>\n",
       "      <td>10336.26</td>\n",
       "      <td>10398.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>10438.33</td>\n",
       "      <td>10504.67</td>\n",
       "      <td>10438.33</td>\n",
       "      <td>10473.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>10488.73</td>\n",
       "      <td>10495.24</td>\n",
       "      <td>10408.06</td>\n",
       "      <td>10443.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>10447.11</td>\n",
       "      <td>10489.17</td>\n",
       "      <td>10444.70</td>\n",
       "      <td>10470.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>10494.60</td>\n",
       "      <td>10577.53</td>\n",
       "      <td>10494.60</td>\n",
       "      <td>10538.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>10517.21</td>\n",
       "      <td>10520.66</td>\n",
       "      <td>10436.11</td>\n",
       "      <td>10491.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>10478.74</td>\n",
       "      <td>10526.41</td>\n",
       "      <td>10472.39</td>\n",
       "      <td>10506.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>10508.33</td>\n",
       "      <td>10544.00</td>\n",
       "      <td>10452.08</td>\n",
       "      <td>10467.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>10470.36</td>\n",
       "      <td>10514.31</td>\n",
       "      <td>10470.36</td>\n",
       "      <td>10504.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>10526.97</td>\n",
       "      <td>10545.78</td>\n",
       "      <td>10488.97</td>\n",
       "      <td>10488.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>10503.10</td>\n",
       "      <td>10537.27</td>\n",
       "      <td>10486.52</td>\n",
       "      <td>10537.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>10546.64</td>\n",
       "      <td>10556.65</td>\n",
       "      <td>10514.54</td>\n",
       "      <td>10522.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>10526.14</td>\n",
       "      <td>10528.92</td>\n",
       "      <td>10408.12</td>\n",
       "      <td>10421.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>10419.81</td>\n",
       "      <td>10497.77</td>\n",
       "      <td>10419.81</td>\n",
       "      <td>10486.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>736</th>\n",
       "      <td>10513.29</td>\n",
       "      <td>10592.04</td>\n",
       "      <td>10513.29</td>\n",
       "      <td>10567.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>10590.95</td>\n",
       "      <td>10659.98</td>\n",
       "      <td>10590.95</td>\n",
       "      <td>10642.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>738 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Open      High       Low     Price\n",
       "0     9292.31   9292.31   9182.02   9274.11\n",
       "1     9209.93   9209.93   9043.44   9048.34\n",
       "2     9051.94   9108.66   9050.54   9080.09\n",
       "3     9154.03   9246.62   9154.03   9238.03\n",
       "4     9247.40   9284.57   9215.58   9215.58\n",
       "5     9198.02   9229.65   9178.30   9178.30\n",
       "6     9162.77   9253.82   9161.71   9231.80\n",
       "7     9235.90   9242.20   9161.71   9180.23\n",
       "8     9197.32   9219.43   9149.10   9165.09\n",
       "9     9245.77   9256.86   9109.46   9138.29\n",
       "10    9183.76   9247.09   9151.82   9174.06\n",
       "11    9188.76   9259.31   9177.48   9251.69\n",
       "12    9283.79   9319.71   9275.68   9319.71\n",
       "13    9336.81   9386.56   9336.81   9369.51\n",
       "14    9422.00   9471.80   9422.00   9470.94\n",
       "15    9474.56   9478.53   9436.47   9477.67\n",
       "16    9505.56   9523.73   9471.19   9521.59\n",
       "17    9503.20   9515.29   9471.31   9510.92\n",
       "18    9479.36   9482.13   9403.75   9426.90\n",
       "19    9452.85   9466.65   9361.91   9361.91\n",
       "20    9368.83   9410.43   9352.19   9386.99\n",
       "21    9394.47   9460.17   9394.47   9448.73\n",
       "22    9477.52   9533.88   9477.52   9513.92\n",
       "23    9511.06   9517.42   9472.42   9512.05\n",
       "24    9521.88   9521.88   9448.65   9456.18\n",
       "25    9465.12   9465.12   9413.18   9421.50\n",
       "26    9405.59   9435.07   9390.13   9393.70\n",
       "27    9430.17   9488.72   9430.17   9462.22\n",
       "28    9496.39   9500.35   9447.19   9496.31\n",
       "29    9500.04   9542.09   9499.97   9529.51\n",
       "..        ...       ...       ...       ...\n",
       "708  10706.79  10728.39  10656.18  10664.55\n",
       "709  10688.22  10787.58  10687.26  10779.24\n",
       "710  10815.40  10882.65  10801.10  10822.59\n",
       "711  10837.61  10872.29  10809.09  10854.57\n",
       "712  10847.79  10873.91  10833.78  10854.09\n",
       "713  10836.82  10856.00  10746.74  10750.93\n",
       "714  10733.56  10752.58  10692.53  10707.07\n",
       "715  10735.24  10762.91  10697.62  10713.55\n",
       "716  10623.20  10650.18  10560.44  10560.44\n",
       "717  10591.62  10665.92  10495.15  10600.37\n",
       "718  10617.27  10665.43  10565.68  10651.11\n",
       "719  10607.62  10635.71  10540.74  10566.85\n",
       "720  10541.03  10541.03  10376.65  10393.92\n",
       "721  10404.42  10418.97  10322.76  10355.76\n",
       "722  10406.35  10419.28  10336.26  10398.62\n",
       "723  10438.33  10504.67  10438.33  10473.09\n",
       "724  10488.73  10495.24  10408.06  10443.28\n",
       "725  10447.11  10489.17  10444.70  10470.70\n",
       "726  10494.60  10577.53  10494.60  10538.01\n",
       "727  10517.21  10520.66  10436.11  10491.44\n",
       "728  10478.74  10526.41  10472.39  10506.52\n",
       "729  10508.33  10544.00  10452.08  10467.34\n",
       "730  10470.36  10514.31  10470.36  10504.52\n",
       "731  10526.97  10545.78  10488.97  10488.97\n",
       "732  10503.10  10537.27  10486.52  10537.27\n",
       "733  10546.64  10556.65  10514.54  10522.49\n",
       "734  10526.14  10528.92  10408.12  10421.91\n",
       "735  10419.81  10497.77  10419.81  10486.67\n",
       "736  10513.29  10592.04  10513.29  10567.64\n",
       "737  10590.95  10659.98  10590.95  10642.86\n",
       "\n",
       "[738 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#整理數據\n",
    "changevalue=x_train.max() #result:0.580645144\n",
    "#changevalue=12000  #result:0.419354826\n",
    "x_train=x_train.reshape(x_train.shape[0], -1) / changevalue\n",
    "x_test=x_test.reshape(x_test.shape[0], -1) / x_test.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85386464,  0.85386464,  0.84373016,  0.85219225,  0.84629479],\n",
       "       [ 0.84629479,  0.84629479,  0.83099613,  0.83144638,  0.83177719],\n",
       "       [ 0.83177719,  0.83698915,  0.83164854,  0.83436387,  0.84115817],\n",
       "       ..., \n",
       "       [ 0.96724052,  0.96749597,  0.95639573,  0.95766289,  0.95746992],\n",
       "       [ 0.95746992,  0.96463361,  0.95746992,  0.96361364,  0.96605974],\n",
       "       [ 0.96605974,  0.97329603,  0.96605974,  0.97105393,  0.97319587]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#隱藏層\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None):\n",
    "    #name_scope:每個運算元命名\n",
    "    with tf.name_scope('Layer'):\n",
    "        with tf.name_scope('Weights'):\n",
    "            # in_size * out_sieze 矩陣\n",
    "            Weights = tf.Variable(tf.random_normal([in_size, out_size]),name=\"Weights\")\n",
    "        with tf.name_scope('Biases'):\n",
    "            # 1 * out_size 矩陣\n",
    "            biases = tf.Variable(tf.zeros([1, out_size]) + 0.1,name=\"biases\")\n",
    "        with tf.name_scope('Formula'):\n",
    "            ii=tf.cast(inputs, tf.float32)\n",
    "            Wx_plus_b = tf.matmul(ii, Weights) + biases\n",
    "        if activation_function is None:\n",
    "            outputs = Wx_plus_b\n",
    "        else:\n",
    "            outputs = activation_function(Wx_plus_b)\n",
    "        return outputs,Weights,biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Inputs'):\n",
    "    xs = tf.placeholder(tf.float32, [None, featurenum+1])\n",
    "    ys = tf.placeholder(tf.float32,[None,1])\n",
    "l1,wi,bi = add_layer(xs, featurenum+1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer\n",
    "prediction,wo,bo = add_layer(l1, 10, 1, activation_function=tf.nn.relu)\n",
    "\n",
    "# the error between prediciton and real data\n",
    "#建構損失函數\n",
    "with tf.name_scope('Loss'):\n",
    "    mse = tf.reduce_mean(tf.squared_difference(prediction,ys)) \n",
    "    cost_loss = tf.reduce_sum(tf.square(ys-prediction)) \n",
    "    tf.summary.scalar('loss', cost_loss)\n",
    "#運用梯度下降法\n",
    "with tf.name_scope('Optimizer'):\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.001).minimize(mse)\n",
    "    \n",
    "correct_pred = tf.equal(tf.argmax(ys, 1), tf.argmax(prediction, 1))\n",
    "acc_op = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001, total loss=195.192636700\n",
      "Epoch: 0006, total loss=194.027882030\n",
      "Accuracy on validation set: 1.000000000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # training loop\n",
    "    for epoch in range(10):\n",
    "        total_loss = 0.\n",
    "        for i in range(len(x_train)):\n",
    "            # prepare feed data and run\n",
    "            feed_dict = {xs: [x_train[i]], ys: [y_train[i]]}\n",
    "            _, loss,wi1,wo2,bi1,bo2 = sess.run([train_step, mse,wi,wi,bo,bo], feed_dict=feed_dict)\n",
    "            total_loss += loss\n",
    "        # display loss per epoch\n",
    "        if epoch % 5==0:\n",
    "            print('Epoch: %04d, total loss=%.9f' % (epoch + 1, total_loss))\n",
    "    accuracy = sess.run(acc_op, feed_dict={xs: x_train,ys:y_train}) \n",
    "    print(\"Accuracy on validation set: %.9f\" % accuracy) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
